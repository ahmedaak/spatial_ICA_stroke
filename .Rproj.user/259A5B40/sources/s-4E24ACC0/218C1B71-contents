# this script performs ELASTIC NET regression using a model (balanced) dataset for training and ...
# then tests it on an unbalanced dataset (1000x cross-validation)
rm(list = ls())
library(glmnet)
library(irr) # needed to calculate Cohen's kappa
library(pROC)
setwd("C:/Users/khalila/Desktop/Manuscripts/sICA_manuscript/analysis/")
# read features
HFE_feats <- read.csv("HFE_feats.csv", header = FALSE)
names(HFE_feats) <- c("Delay_WB", "COV_WB", "Delay_SSStemp", "COV_SSStemp", "Delay_SSSIC", "COV_SSSIC", "%_0_0.01", "%_0.01_0.025", "%_0.025_0.05", "%_0.05_0.1", "%_0.1_0.15", "%_0.15_0.2", "%_VT", "Mean_Tmax","HIC")
HFE_feats <- data.matrix(HFE_feats, rownames.force = NA)
# read class labels
HFE_classlabels <- read.csv("HFE_classLabels.csv", header = FALSE)
HFE_classlabels <- data.matrix(HFE_classlabels, rownames.force = NA)
names(HFE_classlabels) <- c("Patient_No", "IC_No", "HIC")
# remove 2x features with NaN (delay + cov with SSS-IC)
HFE_feats <- HFE_feats[,-5]
HFE_feats <- HFE_feats[,-5]
# correct Tmax units (multiply by 0.1)
HFE_feats[,12] <- HFE_feats[,12]*0.1
# define vectors to save each (across iterations) result - NB "all" refers to all iterations of the testing phase unless otherwise specified
lambdas_all <- c() # all lambdas
coef_all <- c() # all coefficients
auc_all <- c() # all AUCs
acc_all <- c() # all accuracy values
roc_sens_all <- c() # all ROC sensitivities (all thresholds)
roc_spec_all <- c() # all ROC specificities (all thresholds)
opt_t_all <- c() # all optimal thresholds from ROC analysis (maximizes sens + spec)
opt_sens_all <- c() # all optimal sensitivities
opt_spec_all <- c() # all optimal specificities
kappa_all <- c() # all Cohen's kappas
mse_all <- c() # all MSEs

# CREATE MODEL (BALANCED) DATASET + TRAIN MODEL ON IT
# get logical vector of rows corresponding to HICs
hics <- HFE_feats[,ncol(HFE_feats)]==1
# select the features of rows corresponding to HICs
hic_feats_only <- HFE_feats[hics,]
# select the features of rows corresponding to non-HICs
nonhic_feats_only <- HFE_feats[!hics,]
# select a sample of non-HICs equal to n of HICs (indices, then features)
nonhic_feats_sampleindices <- sample(x = nrow(nonhic_feats_only), size = nrow(hic_feats_only), replace = FALSE)
nonhic_feats_sample <- nonhic_feats_only[nonhic_feats_sampleindices,]
# combine the HICs + nonHICs features
model_data_feats <- rbind(hic_feats_only, nonhic_feats_sample)
# split model data into training and test
# select 80% of data as sample size for training
model_train_n <- floor(0.8*nrow(model_data_feats))
# find training data indices
model_train_ind<-sample(seq_len(nrow(model_data_feats)),size=model_train_n) 
# select training data
model_train_feats <- model_data_feats[model_train_ind,]
# select test data
model_test_feats <- model_data_feats[-model_train_ind,]
# train ELASTIC NET GLM on MODEL data
excl_var <-  c(1,2,8,9,10,12,ncol(model_train_feats)) # variables to EXCLUDE from model (last col always)
# fit model
# get lambda WARNING: IS "MSE" THE CORRECT LOSS FUNC TO USE HERE??? + THIS IS LOO CV
lasso_HFE_crossval <- cv.glmnet(x = model_train_feats[,-excl_var], nfolds = nrow(model_train_feats), y = model_train_feats[,13], family = "binomial", alpha = 0.5, type.measure = "mse") 
# get the MSE corresponding to the minimum lambda according to http://bit.ly/2kW0aII
mse.min <- lasso_HFE_crossval$cvm[lasso_HFE_crossval$lambda == lasso_HFE_crossval$lambda.min]
# get coefficients
lasso_HFE_coef <- coef.glmnet(object = lasso_HFE_crossval, s = lasso_HFE_crossval$lambda.min)
# put coefficients into a proper vector
lasso_HFE_coef <- unname(lasso_HFE_coef, force = FALSE) # unname matrix
lasso_HFE_coef <- as.vector(lasso_HFE_coef) # turn matrix into a vector

for (i in 1:10000) {
  # CREATE UNBALANCED (REALISTIC) TEST DATASET + VALIDATE MODEL ON IT
  # select a sample of HICs (indices) that were NOT included in the training dataset (n=5 out of possible 15)
  hic_feats_test <- model_test_feats[,ncol(model_test_feats)]==1
  hic_feats_test_data <- model_test_feats[hic_feats_test,]
  hic_feats_sampleindices_test <- sample(x = nrow(hic_feats_test_data), size = 5, replace = FALSE)
  hic_feats_sample_test <- hic_feats_test_data[hic_feats_sampleindices_test,]
  # select a sample of non-HICs equal to 10*n of HICs (indices, then features) that were NOT included in training
  nonhic_feats_only_excl_training <- nonhic_feats_only[-nonhic_feats_sampleindices,] # exclude data used during training
  nonhic_feats_sampleindices_test <- sample(x = nrow(nonhic_feats_only_excl_training), size = 10*nrow(hic_feats_test_data), replace = FALSE)
  nonhic_feats_sample_test <- nonhic_feats_only_excl_training[nonhic_feats_sampleindices_test,]
  # combine the HICs + nonHICs features
  real_data_feats <- rbind(hic_feats_test_data, nonhic_feats_sample_test)
  
  # make predictions on REALISTIC TEST data
  real_test_predict <- predict(object = lasso_HFE_crossval, newx = real_data_feats[,-excl_var], s = lasso_HFE_crossval$lambda.min, type = "response")
  real_test_predict <- as.vector(real_test_predict) # if I don't do this, "roc" gives weird warning to pass a vector instead of matrix (still runs tho)
  # assess the performance of the model on the REALISTIC TEST data
  test.class<-real_data_feats[,13] # The true class, i.e. outcome
  # Assess performance in terms of AUC for training set:
  test.rocurve<-roc(response=test.class, predictor=real_test_predict) # calculate a ROC curve
  test.AUC<-test.rocurve$auc #+auc(roc=rocurve)
  #cat("Performance on test set: AUC=",test.AUC,"\n") 
  # Find threshold (t) that minimizes error according to http://bit.ly/2k6UgCG 
  e <- cbind(test.rocurve$thresholds,test.rocurve$sensitivities+test.rocurve$specificities)
  opt_t <- subset(e,e[,2]==max(e[,2]))[,1]
  # find sensitivity + specificity at t
  # find row of t
  row_t <- which(x = e[,1]==opt_t)
  # match to row in sens/spec vector
  opt_sens <- test.rocurve$sensitivities[row_t]
  opt_spec <- test.rocurve$specificities[row_t]
  # define predicted classes based on threshold (t)
  test.prediction.class = real_test_predict>opt_t
  # Assess performance in terms of accuracy (%) for training set:
  test.accuracy <- 1-sum(xor(test.prediction.class, test.class))/nrow(real_data_feats)
  cmat = table(test.class, test.prediction.class) # confusion matrix (contingency table)
  # Assess performance in terms of Cohen's kappa for test set: Why? because it's insensitive to class imbalance
  test_kappa <- kappa2(cbind(test.class, test.prediction.class))$value
  #cat('accuracy on test set:',100*test.accuracy, '%')
  # save all results (across iterations)
  auc_all <- append(auc_all, test.AUC)
  lambdas_all <- append(lambdas_all, lasso_HFE_crossval$lambda.min)
  coef_all <- cbind(coef_all, lasso_HFE_coef) 
  rownames(coef_all) <- c("intercept", colnames(real_data_feats[,-excl_var])) # put the variable names back
  OR_all <- exp(coef_all) # convert coefficients to odds ratios
  acc_all <- append(acc_all, test.accuracy)
  roc_sens_all <- cbind(roc_sens_all, test.rocurve$sensitivities)
  roc_spec_all <- cbind (roc_spec_all, test.rocurve$specificities)
  opt_t_all <- append(opt_t_all, opt_t)
  opt_sens_all <- append(opt_sens_all, opt_sens)
  opt_spec_all <- append(opt_spec_all, opt_spec)
  kappa_all <- append(kappa_all, test_kappa)
  mse_all <- append(mse_all, mse.min)}
# plot all ROC curves, one on top of the other
matplot(1-roc_spec_all, roc_sens_all, type = "l", lwd = "2", lty = 1)
abline(a = 0, b = 1)
# report main results
cat("Median performance on real test set: AUC=",median(auc_all),"\n")
cat("Median accuracy on real test set: ",median(acc_all),"\n")
cat("Median optimal sensitivity on real test set: ", median(opt_sens_all), "\n")
cat("Median optimal specificity on real test set: ", median(opt_spec_all), "\n")
cat("Median Cohen's kappa on real test set: ", median(kappa_all), "\n")